{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from timm.models.layers import trunc_normal_ as __call_trunc_normal_\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from timm.models.layers import trunc_normal_, DropPath, to_2tuple\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import os\n",
    "import torchmetrics\n",
    "import timm\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import datetime\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "import random\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"7\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=4\n",
    "image_count=25\n",
    "img_size=256\n",
    "num_channels=3\n",
    "tf = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1165/1165 [01:46<00:00, 10.98it/s]\n",
      "100%|██████████| 292/292 [00:20<00:00, 13.97it/s]\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, id,image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "\n",
    "        self.label = label_list\n",
    "        self.id=id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_tensor=self.id[idx]\n",
    "        image_tensor = self.img_path[idx]\n",
    "    \n",
    "        label_tensor =  self.label[idx]\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "\n",
    "train_data=pd.read_csv('../../data/mteg_data/internal/1-fold_train.csv') \n",
    "file_path='../../data/mteg_data/internal/all/'\n",
    "train_image_list=[]\n",
    "for i in range(len(train_data)):\n",
    "    file_name=train_data.loc[i]['File Name']\n",
    "    id=file_name[:file_name.find('_')]\n",
    "    train_image_list.append(file_path+id)\n",
    "label_data=pd.read_csv('../../data/mteg_data/internal/label.csv')  \n",
    "train_label_list=[]\n",
    "train_id_list=[]\n",
    "train_image_tensor = torch.empty((len(train_image_list),3,image_count, img_size, img_size))\n",
    "for i in tqdm(range(len(train_image_list))):\n",
    "    folder_name=os.path.basename(train_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    train_id_list.append(folder_name)\n",
    "    train_label_list.append(label-1) \n",
    "    image_file_list = glob(train_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,:,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,:,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count-1):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,:,count] = image\n",
    "            count += 1\n",
    "            \n",
    "\n",
    "test_data=pd.read_csv('../../data/mteg_data/internal/1-fold_test.csv') \n",
    "file_path='../../data/mteg_data/internal/all/'\n",
    "test_image_list=[]\n",
    "for i in range(len(test_data)):\n",
    "    file_name=test_data.loc[i]['File Name']\n",
    "    id=file_name[:file_name.find('_')]\n",
    "    test_image_list.append(file_path+id)\n",
    "label_data=pd.read_csv('../../data/mteg_data/internal/label.csv')  \n",
    "test_label_list=[]\n",
    "test_id_list=[]\n",
    "test_image_tensor = torch.empty((len(test_image_list),3,image_count, img_size, img_size))\n",
    "for i in tqdm(range(len(test_image_list))):\n",
    "    folder_name=os.path.basename(test_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    test_id_list.append(folder_name)\n",
    "    test_label_list.append(label-1) \n",
    "    image_file_list = glob(test_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,:,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,:,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,:,count] = image\n",
    "            count += 1\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_id_list,train_image_tensor, F.one_hot(torch.tensor(train_label_list).to(torch.int64)))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataset = CustomDataset(test_id_list,test_image_tensor, F.one_hot(torch.tensor(test_label_list).to(torch.int64)))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_3xnxn(inp, oup, kernel_size=3, stride=3, groups=1):\n",
    "    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 0, 0), groups=groups)\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups\n",
    "\n",
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, _BatchNorm):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "            \n",
    "def conv_1xnxn(inp, oup, kernel_size=3, stride=3, groups=1):\n",
    "    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 0, 0), groups=groups)\n",
    "\n",
    "def conv_3xnxn_std(inp, oup, kernel_size=3, stride=3, groups=1):\n",
    "    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (1, stride, stride), (1, 0, 0), groups=groups)\n",
    "\n",
    "def conv_1x1x1(inp, oup, groups=1):\n",
    "    return nn.Conv3d(inp, oup, (1, 1, 1), (1, 1, 1), (0, 0, 0), groups=groups)\n",
    "\n",
    "def conv_3x3x3(inp, oup, groups=1):\n",
    "    return nn.Conv3d(inp, oup, (3, 3, 3), (1, 1, 1), (1, 1, 1), groups=groups)\n",
    "\n",
    "def conv_5x5x5(inp, oup, groups=1):\n",
    "    return nn.Conv3d(inp, oup, (5, 5, 5), (1, 1, 1), (2, 2, 2), groups=groups)\n",
    "\n",
    "def bn_3d(dim):\n",
    "    return nn.BatchNorm3d(dim)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CMlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = conv_1x1x1(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = conv_1x1x1(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.pos_embed = conv_3x3x3(dim, dim, groups=dim)\n",
    "        self.norm1 = bn_3d(dim)\n",
    "        self.conv1 = conv_1x1x1(dim, dim, 1)\n",
    "        self.conv2 = conv_1x1x1(dim, dim, 1)\n",
    "        self.attn = conv_5x5x5(dim, dim, groups=dim)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = bn_3d(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = CMlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embed(x)\n",
    "        x = x + self.drop_path(self.conv2(self.attn(self.conv1(self.norm1(x)))))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x   \n",
    "\n",
    "\n",
    "class SABlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.pos_embed = conv_3x3x3(dim, dim, groups=dim)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embed(x)\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        x = x.transpose(1, 2).reshape(B, C, T, H, W)\n",
    "        return x    \n",
    "\n",
    "\n",
    "class SplitSABlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.pos_embed = conv_3x3x3(dim, dim, groups=dim)\n",
    "        self.t_norm = norm_layer(dim)\n",
    "        self.t_attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embed(x)\n",
    "        B, C, T, H, W = x.shape\n",
    "        attn = x.view(B, C, T, H * W).permute(0, 3, 2, 1).contiguous()\n",
    "        attn = attn.view(B * H * W, T, C)\n",
    "        attn = attn + self.drop_path(self.t_attn(self.t_norm(attn)))\n",
    "        attn = attn.view(B, H * W, T, C).permute(0, 2, 1, 3).contiguous()\n",
    "        attn = attn.view(B * T, H * W, C)\n",
    "        residual = x.view(B, C, T, H * W).permute(0, 2, 3, 1).contiguous()\n",
    "        residual = residual.view(B * T, H * W, C)\n",
    "        attn = residual + self.drop_path(self.attn(self.norm1(attn)))\n",
    "        attn = attn.view(B, T * H * W, C)\n",
    "        out = attn + self.drop_path(self.mlp(self.norm2(attn)))\n",
    "        out = out.transpose(1, 2).reshape(B, C, T, H, W)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SpeicalPatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.proj = conv_3xnxn(in_chans, embed_dim, kernel_size=patch_size[0], stride=patch_size[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        # assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, T, H, W, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "    \n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, std=False):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        if std:\n",
    "            self.proj = conv_3xnxn_std(in_chans, embed_dim, kernel_size=patch_size[0], stride=patch_size[0])\n",
    "        else:\n",
    "            self.proj = conv_1xnxn(in_chans, embed_dim, kernel_size=patch_size[0], stride=patch_size[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        # assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, T, H, W, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "\n",
    "class Uniformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`  -\n",
    "        https://arxiv.org/abs/2010.11929\n",
    "    \"\"\"\n",
    "    def __init__(self, depth=[5, 8, 20, 7], num_classes=400, img_size=224, in_chans=3, embed_dim=[64, 128, 320, 512],\n",
    "                 head_dim=64, mlp_ratio=4., qkv_bias=True, qk_scale=None, representation_size=None,\n",
    "                 drop_rate=0.3, attn_drop_rate=0., drop_path_rate=0., norm_layer=None, split=False, std=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        norm_layer = partial(nn.LayerNorm, eps=1e-6) \n",
    "        \n",
    "        self.patch_embed1 = SpeicalPatchEmbed(\n",
    "            img_size=img_size, patch_size=4, in_chans=in_chans, embed_dim=embed_dim[0])\n",
    "        self.patch_embed2 = PatchEmbed(\n",
    "            img_size=img_size // 4, patch_size=2, in_chans=embed_dim[0], embed_dim=embed_dim[1], std=std)\n",
    "        self.patch_embed3 = PatchEmbed(\n",
    "            img_size=img_size // 8, patch_size=2, in_chans=embed_dim[1], embed_dim=embed_dim[2], std=std)\n",
    "        self.patch_embed4 = PatchEmbed(\n",
    "            img_size=img_size // 16, patch_size=2, in_chans=embed_dim[2], embed_dim=embed_dim[3], std=std)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depth))]  # stochastic depth decay rule\n",
    "        num_heads = [dim // head_dim for dim in embed_dim]\n",
    "        self.blocks1 = nn.ModuleList([\n",
    "            CBlock(\n",
    "                dim=embed_dim[0], num_heads=num_heads[0], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth[0])])\n",
    "        self.blocks2 = nn.ModuleList([\n",
    "            CBlock(\n",
    "                dim=embed_dim[1], num_heads=num_heads[1], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+depth[0]], norm_layer=norm_layer)\n",
    "            for i in range(depth[1])])\n",
    "        if split:\n",
    "            self.blocks3 = nn.ModuleList([\n",
    "                SplitSABlock(\n",
    "                    dim=embed_dim[2], num_heads=num_heads[2], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+depth[0]+depth[1]], norm_layer=norm_layer)\n",
    "                for i in range(depth[2])])\n",
    "            self.blocks4 = nn.ModuleList([\n",
    "                SplitSABlock(\n",
    "                    dim=embed_dim[3], num_heads=num_heads[3], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+depth[0]+depth[1]+depth[2]], norm_layer=norm_layer)\n",
    "            for i in range(depth[3])])\n",
    "        else:\n",
    "            self.blocks3 = nn.ModuleList([\n",
    "                SABlock(\n",
    "                    dim=embed_dim[2], num_heads=num_heads[2], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+depth[0]+depth[1]], norm_layer=norm_layer)\n",
    "                for i in range(depth[2])])\n",
    "            self.blocks4 = nn.ModuleList([\n",
    "                SABlock(\n",
    "                    dim=embed_dim[3], num_heads=num_heads[3], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+depth[0]+depth[1]+depth[2]], norm_layer=norm_layer)\n",
    "            for i in range(depth[3])])\n",
    "        self.norm = bn_3d(embed_dim[-1])\n",
    "        \n",
    "        # Representation layer\n",
    "        if representation_size:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "        \n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim[-1], num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        for name, p in self.named_parameters():\n",
    "            # fill proj weight with 1 here to improve training dynamics. Otherwise temporal attention inputs\n",
    "            # are multiplied by 0*0, which is hard for the model to move out of.\n",
    "            if 't_attn.qkv.weight' in name:\n",
    "                nn.init.constant_(p, 0)\n",
    "            if 't_attn.qkv.bias' in name:\n",
    "                nn.init.constant_(p, 0)\n",
    "            if 't_attn.proj.weight' in name:\n",
    "                nn.init.constant_(p, 1)\n",
    "            if 't_attn.proj.bias' in name:\n",
    "                nn.init.constant_(p, 0)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed1(x)\n",
    "        x = self.pos_drop(x)\n",
    "        for blk in self.blocks1:\n",
    "            x = blk(x)\n",
    "        x = self.patch_embed2(x)\n",
    "        for blk in self.blocks2:\n",
    "            x = blk(x)\n",
    "        x = self.patch_embed3(x)\n",
    "        for blk in self.blocks3:\n",
    "            x = blk(x)\n",
    "        x = self.patch_embed4(x)\n",
    "        for blk in self.blocks4:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pre_logits(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = x.flatten(2).mean(-1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "def uniformer_small():\n",
    "    return Uniformer(\n",
    "        depth=[3, 4, 8, 3], embed_dim=[64, 128, 320, 512], \n",
    "        head_dim=64, drop_rate=0.1)\n",
    "\n",
    "def uniformer_base():\n",
    "    return Uniformer(\n",
    "        depth=[5, 8, 20, 7], embed_dim=[64, 128, 320, 512], \n",
    "        head_dim=64, drop_rate=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = [train_label_list.count(0), train_label_list.count(1), train_label_list.count(2)]\n",
    "class_weights = [1 - (x / sum(class_counts)) for x in class_counts]\n",
    "class_weights =  torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes=3).to(device)\n",
    "model=Uniformer(num_classes=3,img_size=224,in_chans=3)\n",
    "model.to(device)\n",
    "base_optimizer = torch.optim.SGD\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=2e-2, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deeplearning Start]\n",
      "deeplearning Start Time : 2024-5-13 17:59:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/300 Step: 292 loss : 1.0882 accuracy: 0.3943: 100%|██████████| 291/291 [09:52<00:00,  2.04s/it]\n",
      "val_epoch: 1/300 Step: 292 loss : 1.0931 accuracy: 0.4041: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 2/300 Step: 292 loss : 1.0842 accuracy: 0.3892: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 2/300 Step: 292 loss : 1.1536 accuracy: 0.1849: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 3/300 Step: 292 loss : 1.0816 accuracy: 0.4278: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 3/300 Step: 292 loss : 1.2195 accuracy: 0.1849: 100%|██████████| 73/73 [00:24<00:00,  2.97it/s]\n",
      "epoch: 4/300 Step: 292 loss : 1.0788 accuracy: 0.4132: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 4/300 Step: 292 loss : 1.1474 accuracy: 0.1849: 100%|██████████| 73/73 [00:24<00:00,  3.00it/s]\n",
      "epoch: 5/300 Step: 292 loss : 1.0808 accuracy: 0.4149: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 5/300 Step: 292 loss : 1.1433 accuracy: 0.2979: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 6/300 Step: 292 loss : 1.0775 accuracy: 0.4244: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 6/300 Step: 292 loss : 1.0690 accuracy: 0.4418: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 7/300 Step: 292 loss : 1.0746 accuracy: 0.4562: 100%|██████████| 291/291 [09:55<00:00,  2.04s/it]\n",
      "val_epoch: 7/300 Step: 292 loss : 1.0604 accuracy: 0.4486: 100%|██████████| 73/73 [00:23<00:00,  3.08it/s]\n",
      "epoch: 8/300 Step: 292 loss : 1.0784 accuracy: 0.4296: 100%|██████████| 291/291 [09:55<00:00,  2.05s/it]\n",
      "val_epoch: 8/300 Step: 292 loss : 1.0872 accuracy: 0.3733: 100%|██████████| 73/73 [00:24<00:00,  3.00it/s]\n",
      "epoch: 9/300 Step: 292 loss : 1.0711 accuracy: 0.4545: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 9/300 Step: 292 loss : 1.1215 accuracy: 0.4384: 100%|██████████| 73/73 [00:24<00:00,  3.04it/s]\n",
      "epoch: 10/300 Step: 292 loss : 1.0709 accuracy: 0.4502: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 10/300 Step: 292 loss : 1.0814 accuracy: 0.4349: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 11/300 Step: 292 loss : 1.0691 accuracy: 0.4562: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 11/300 Step: 292 loss : 1.1183 accuracy: 0.4281: 100%|██████████| 73/73 [00:24<00:00,  3.00it/s]\n",
      "epoch: 12/300 Step: 292 loss : 1.0693 accuracy: 0.4631: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 12/300 Step: 292 loss : 1.1079 accuracy: 0.4418: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 13/300 Step: 292 loss : 1.0639 accuracy: 0.4656: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 13/300 Step: 292 loss : 1.1116 accuracy: 0.4384: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 14/300 Step: 292 loss : 1.0678 accuracy: 0.4527: 100%|██████████| 291/291 [09:55<00:00,  2.05s/it]\n",
      "val_epoch: 14/300 Step: 292 loss : 1.1188 accuracy: 0.4486: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 15/300 Step: 292 loss : 1.0657 accuracy: 0.4605: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 15/300 Step: 292 loss : 1.1046 accuracy: 0.4486: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 16/300 Step: 292 loss : 1.0611 accuracy: 0.4613: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 16/300 Step: 292 loss : 1.1297 accuracy: 0.4349: 100%|██████████| 73/73 [00:24<00:00,  2.99it/s]\n",
      "epoch: 17/300 Step: 292 loss : 1.0591 accuracy: 0.4682: 100%|██████████| 291/291 [09:55<00:00,  2.04s/it]\n",
      "val_epoch: 17/300 Step: 292 loss : 1.0964 accuracy: 0.4589: 100%|██████████| 73/73 [00:23<00:00,  3.05it/s]\n",
      "epoch: 18/300 Step: 292 loss : 1.0601 accuracy: 0.4579: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 18/300 Step: 292 loss : 1.1352 accuracy: 0.4315: 100%|██████████| 73/73 [00:24<00:00,  3.01it/s]\n",
      "epoch: 19/300 Step: 292 loss : 1.0611 accuracy: 0.4570: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 19/300 Step: 292 loss : 1.1549 accuracy: 0.4212: 100%|██████████| 73/73 [00:24<00:00,  3.00it/s]\n",
      "epoch: 20/300 Step: 292 loss : 1.0625 accuracy: 0.4613: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 20/300 Step: 292 loss : 1.1669 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.04it/s]\n",
      "epoch: 21/300 Step: 292 loss : 1.0579 accuracy: 0.4699: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 21/300 Step: 292 loss : 1.1681 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.04it/s]\n",
      "epoch: 22/300 Step: 292 loss : 1.0614 accuracy: 0.4570: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 22/300 Step: 292 loss : 1.1205 accuracy: 0.4418: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 23/300 Step: 292 loss : 1.0553 accuracy: 0.4674: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 23/300 Step: 292 loss : 1.1647 accuracy: 0.4144: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 24/300 Step: 292 loss : 1.0503 accuracy: 0.4845: 100%|██████████| 291/291 [09:55<00:00,  2.04s/it]\n",
      "val_epoch: 24/300 Step: 292 loss : 1.1583 accuracy: 0.4178: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 25/300 Step: 292 loss : 1.0518 accuracy: 0.4794: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 25/300 Step: 292 loss : 1.1418 accuracy: 0.4281: 100%|██████████| 73/73 [00:23<00:00,  3.05it/s]\n",
      "epoch: 26/300 Step: 292 loss : 1.0478 accuracy: 0.4768: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 26/300 Step: 292 loss : 1.1675 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 27/300 Step: 292 loss : 1.0579 accuracy: 0.4527: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 27/300 Step: 292 loss : 1.1572 accuracy: 0.4212: 100%|██████████| 73/73 [00:24<00:00,  2.98it/s]\n",
      "epoch: 28/300 Step: 292 loss : 1.0479 accuracy: 0.4759: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 28/300 Step: 292 loss : 1.1643 accuracy: 0.4144: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 29/300 Step: 292 loss : 1.0419 accuracy: 0.4734: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 29/300 Step: 292 loss : 1.1678 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 30/300 Step: 292 loss : 1.0453 accuracy: 0.4777: 100%|██████████| 291/291 [09:55<00:00,  2.05s/it]\n",
      "val_epoch: 30/300 Step: 292 loss : 1.1589 accuracy: 0.4144: 100%|██████████| 73/73 [00:23<00:00,  3.06it/s]\n",
      "epoch: 31/300 Step: 292 loss : 1.0459 accuracy: 0.4820: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 31/300 Step: 292 loss : 1.1675 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.04it/s]\n",
      "epoch: 32/300 Step: 292 loss : 1.0434 accuracy: 0.4931: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 32/300 Step: 292 loss : 1.1673 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  2.99it/s]\n",
      "epoch: 33/300 Step: 292 loss : 1.0307 accuracy: 0.4983: 100%|██████████| 291/291 [09:55<00:00,  2.04s/it]\n",
      "val_epoch: 33/300 Step: 292 loss : 1.1679 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 34/300 Step: 292 loss : 1.0410 accuracy: 0.4802: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 34/300 Step: 292 loss : 1.1286 accuracy: 0.4349: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 35/300 Step: 292 loss : 1.0469 accuracy: 0.4768: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 35/300 Step: 292 loss : 1.1666 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.04it/s]\n",
      "epoch: 36/300 Step: 292 loss : 1.0349 accuracy: 0.5000: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 36/300 Step: 292 loss : 1.1682 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 37/300 Step: 292 loss : 1.0340 accuracy: 0.4974: 100%|██████████| 291/291 [09:55<00:00,  2.04s/it]\n",
      "val_epoch: 37/300 Step: 292 loss : 1.1052 accuracy: 0.4521: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 38/300 Step: 292 loss : 1.0357 accuracy: 0.4983: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 38/300 Step: 292 loss : 1.1290 accuracy: 0.4178: 100%|██████████| 73/73 [00:24<00:00,  3.01it/s]\n",
      "epoch: 39/300 Step: 292 loss : 1.0263 accuracy: 0.5112: 100%|██████████| 291/291 [09:55<00:00,  2.05s/it]\n",
      "val_epoch: 39/300 Step: 292 loss : 1.1640 accuracy: 0.4075: 100%|██████████| 73/73 [00:24<00:00,  3.00it/s]\n",
      "epoch: 40/300 Step: 292 loss : 1.0324 accuracy: 0.4974: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 40/300 Step: 292 loss : 1.1676 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.01it/s]\n",
      "epoch: 41/300 Step: 292 loss : 1.0319 accuracy: 0.4991: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 41/300 Step: 292 loss : 1.1155 accuracy: 0.4521: 100%|██████████| 73/73 [00:24<00:00,  3.01it/s]\n",
      "epoch: 42/300 Step: 292 loss : 1.0241 accuracy: 0.5112: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 42/300 Step: 292 loss : 1.1613 accuracy: 0.4178: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 43/300 Step: 292 loss : 1.0314 accuracy: 0.5000: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 43/300 Step: 292 loss : 1.1440 accuracy: 0.4349: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 44/300 Step: 292 loss : 1.0159 accuracy: 0.5309: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 44/300 Step: 292 loss : 1.1178 accuracy: 0.4486: 100%|██████████| 73/73 [00:23<00:00,  3.04it/s]\n",
      "epoch: 45/300 Step: 292 loss : 1.0257 accuracy: 0.5043: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 45/300 Step: 292 loss : 1.1523 accuracy: 0.4247: 100%|██████████| 73/73 [00:24<00:00,  2.99it/s]\n",
      "epoch: 46/300 Step: 292 loss : 1.0223 accuracy: 0.5112: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 46/300 Step: 292 loss : 1.1176 accuracy: 0.4521: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 47/300 Step: 292 loss : 1.0102 accuracy: 0.5387: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 47/300 Step: 292 loss : 1.1624 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.04it/s]\n",
      "epoch: 48/300 Step: 292 loss : 1.0050 accuracy: 0.5473: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 48/300 Step: 292 loss : 1.1369 accuracy: 0.4178: 100%|██████████| 73/73 [00:24<00:00,  3.01it/s]\n",
      "epoch: 49/300 Step: 292 loss : 1.0167 accuracy: 0.5163: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 49/300 Step: 292 loss : 1.1677 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.01it/s]\n",
      "epoch: 50/300 Step: 292 loss : 1.0221 accuracy: 0.5215: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 50/300 Step: 292 loss : 1.1696 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 51/300 Step: 292 loss : 1.0078 accuracy: 0.5421: 100%|██████████| 291/291 [09:55<00:00,  2.04s/it]\n",
      "val_epoch: 51/300 Step: 292 loss : 1.1704 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.00it/s]\n",
      "epoch: 52/300 Step: 292 loss : 1.0000 accuracy: 0.5567: 100%|██████████| 291/291 [09:55<00:00,  2.04s/it]\n",
      "val_epoch: 52/300 Step: 292 loss : 1.1601 accuracy: 0.4178: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 53/300 Step: 292 loss : 1.0061 accuracy: 0.5369: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 53/300 Step: 292 loss : 1.1529 accuracy: 0.4247: 100%|██████████| 73/73 [00:24<00:00,  3.04it/s]\n",
      "epoch: 54/300 Step: 292 loss : 1.0071 accuracy: 0.5352: 100%|██████████| 291/291 [09:55<00:00,  2.05s/it]\n",
      "val_epoch: 54/300 Step: 292 loss : 1.1500 accuracy: 0.4247: 100%|██████████| 73/73 [00:24<00:00,  3.00it/s]\n",
      "epoch: 55/300 Step: 292 loss : 1.0128 accuracy: 0.5344: 100%|██████████| 291/291 [09:55<00:00,  2.04s/it]\n",
      "val_epoch: 55/300 Step: 292 loss : 1.1658 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 56/300 Step: 292 loss : 0.9922 accuracy: 0.5558: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 56/300 Step: 292 loss : 1.1415 accuracy: 0.4349: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 57/300 Step: 292 loss : 1.0053 accuracy: 0.5387: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 57/300 Step: 292 loss : 1.1462 accuracy: 0.4281: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 58/300 Step: 292 loss : 0.9981 accuracy: 0.5541: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 58/300 Step: 292 loss : 1.1344 accuracy: 0.4418: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 59/300 Step: 292 loss : 0.9906 accuracy: 0.5567: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 59/300 Step: 292 loss : 1.1739 accuracy: 0.4075: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 60/300 Step: 292 loss : 0.9911 accuracy: 0.5576: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 60/300 Step: 292 loss : 1.1662 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 61/300 Step: 292 loss : 0.9800 accuracy: 0.5662: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 61/300 Step: 292 loss : 1.0921 accuracy: 0.4829: 100%|██████████| 73/73 [00:23<00:00,  3.05it/s]\n",
      "epoch: 62/300 Step: 292 loss : 0.9684 accuracy: 0.5790: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 62/300 Step: 292 loss : 1.1567 accuracy: 0.4247: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 63/300 Step: 292 loss : 0.9883 accuracy: 0.5550: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 63/300 Step: 292 loss : 1.1540 accuracy: 0.4144: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 64/300 Step: 292 loss : 0.9837 accuracy: 0.5662: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 64/300 Step: 292 loss : 1.1680 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 65/300 Step: 292 loss : 0.9821 accuracy: 0.5670: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 65/300 Step: 292 loss : 1.1572 accuracy: 0.4178: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 66/300 Step: 292 loss : 0.9761 accuracy: 0.5799: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 66/300 Step: 292 loss : 1.1241 accuracy: 0.4486: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 67/300 Step: 292 loss : 0.9794 accuracy: 0.5790: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 67/300 Step: 292 loss : 1.1561 accuracy: 0.4212: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 68/300 Step: 292 loss : 0.9844 accuracy: 0.5593: 100%|██████████| 291/291 [09:55<00:00,  2.04s/it]\n",
      "val_epoch: 68/300 Step: 292 loss : 1.1681 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.00it/s]\n",
      "epoch: 69/300 Step: 292 loss : 0.9757 accuracy: 0.5782: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 69/300 Step: 292 loss : 1.1677 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.04it/s]\n",
      "epoch: 70/300 Step: 292 loss : 0.9747 accuracy: 0.5644: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 70/300 Step: 292 loss : 1.1468 accuracy: 0.4247: 100%|██████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "epoch: 71/300 Step: 292 loss : 0.9744 accuracy: 0.5610: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 71/300 Step: 292 loss : 1.1681 accuracy: 0.4110: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 72/300 Step: 292 loss : 0.9761 accuracy: 0.5696: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 72/300 Step: 292 loss : 1.1473 accuracy: 0.4212: 100%|██████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "epoch: 73/300 Step: 292 loss : 0.9780 accuracy: 0.5739: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 73/300 Step: 292 loss : 1.1658 accuracy: 0.4144: 100%|██████████| 73/73 [00:24<00:00,  3.04it/s]\n",
      "epoch: 74/300 Step: 292 loss : 0.9605 accuracy: 0.5885: 100%|██████████| 291/291 [09:54<00:00,  2.04s/it]\n",
      "val_epoch: 74/300 Step: 292 loss : 1.1654 accuracy: 0.4144: 100%|██████████| 73/73 [00:23<00:00,  3.06it/s]\n",
      "epoch: 75/300 Step: 292 loss : 0.9529 accuracy: 0.6005: 100%|██████████| 291/291 [09:52<00:00,  2.04s/it]\n",
      "val_epoch: 75/300 Step: 292 loss : 1.1617 accuracy: 0.4178: 100%|██████████| 73/73 [00:23<00:00,  3.06it/s]\n",
      "epoch: 76/300 Step: 292 loss : 0.9554 accuracy: 0.6014: 100%|██████████| 291/291 [09:52<00:00,  2.04s/it]\n",
      "val_epoch: 76/300 Step: 292 loss : 1.1675 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.06it/s]\n",
      "epoch: 77/300 Step: 292 loss : 0.9589 accuracy: 0.5936: 100%|██████████| 291/291 [09:52<00:00,  2.04s/it]\n",
      "val_epoch: 77/300 Step: 292 loss : 1.1446 accuracy: 0.4315: 100%|██████████| 73/73 [00:23<00:00,  3.08it/s]\n",
      "epoch: 78/300 Step: 292 loss : 0.9694 accuracy: 0.5756: 100%|██████████| 291/291 [09:53<00:00,  2.04s/it]\n",
      "val_epoch: 78/300 Step: 292 loss : 1.1645 accuracy: 0.4144: 100%|██████████| 73/73 [00:23<00:00,  3.06it/s]\n",
      "epoch: 79/300 Step: 292 loss : 0.9362 accuracy: 0.6143: 100%|██████████| 291/291 [09:53<00:00,  2.04s/it]\n",
      "val_epoch: 79/300 Step: 292 loss : 1.1678 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.07it/s]\n",
      "epoch: 80/300 Step: 292 loss : 0.9407 accuracy: 0.6143: 100%|██████████| 291/291 [09:53<00:00,  2.04s/it]\n",
      "val_epoch: 80/300 Step: 292 loss : 1.1512 accuracy: 0.4247: 100%|██████████| 73/73 [00:23<00:00,  3.06it/s]\n",
      "epoch: 81/300 Step: 292 loss : 0.9415 accuracy: 0.6151: 100%|██████████| 291/291 [09:52<00:00,  2.04s/it]\n",
      "val_epoch: 81/300 Step: 292 loss : 1.1668 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.07it/s]\n",
      "epoch: 82/300 Step: 292 loss : 0.9439 accuracy: 0.6048: 100%|██████████| 291/291 [09:53<00:00,  2.04s/it]\n",
      "val_epoch: 82/300 Step: 292 loss : 1.1643 accuracy: 0.4144: 100%|██████████| 73/73 [00:23<00:00,  3.07it/s]\n",
      "epoch: 83/300 Step: 292 loss : 1.0221 accuracy: 0.5129: 100%|██████████| 291/291 [09:52<00:00,  2.04s/it]\n",
      "val_epoch: 83/300 Step: 292 loss : 1.1681 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.07it/s]\n",
      "epoch: 84/300 Step: 292 loss : 1.0895 accuracy: 0.4141: 100%|██████████| 291/291 [09:51<00:00,  2.03s/it]\n",
      "val_epoch: 84/300 Step: 292 loss : 1.1650 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.07it/s]\n",
      "epoch: 85/300 Step: 292 loss : 1.0864 accuracy: 0.3986: 100%|██████████| 291/291 [09:51<00:00,  2.03s/it]\n",
      "val_epoch: 85/300 Step: 292 loss : 1.1504 accuracy: 0.4041: 100%|██████████| 73/73 [00:23<00:00,  3.08it/s]\n",
      "epoch: 86/300 Step: 292 loss : 1.0824 accuracy: 0.4115: 100%|██████████| 291/291 [09:51<00:00,  2.03s/it]\n",
      "val_epoch: 86/300 Step: 292 loss : 1.1660 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.06it/s]\n",
      "epoch: 87/300 Step: 292 loss : 1.0838 accuracy: 0.4089: 100%|██████████| 291/291 [09:51<00:00,  2.03s/it]\n",
      "val_epoch: 87/300 Step: 292 loss : 1.1654 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.08it/s]\n",
      "epoch: 88/300 Step: 292 loss : 1.0837 accuracy: 0.3943: 100%|██████████| 291/291 [09:51<00:00,  2.03s/it]\n",
      "val_epoch: 88/300 Step: 292 loss : 1.1671 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.07it/s]\n",
      "epoch: 89/300 Step: 292 loss : 1.0826 accuracy: 0.3960: 100%|██████████| 291/291 [09:51<00:00,  2.03s/it]\n",
      "val_epoch: 89/300 Step: 292 loss : 1.1503 accuracy: 0.4041: 100%|██████████| 73/73 [00:23<00:00,  3.08it/s]\n",
      "epoch: 90/300 Step: 292 loss : 1.0832 accuracy: 0.4081: 100%|██████████| 291/291 [09:50<00:00,  2.03s/it]\n",
      "val_epoch: 90/300 Step: 292 loss : 1.1519 accuracy: 0.4041: 100%|██████████| 73/73 [00:23<00:00,  3.08it/s]\n",
      "epoch: 91/300 Step: 292 loss : 1.0828 accuracy: 0.3875: 100%|██████████| 291/291 [09:51<00:00,  2.03s/it]\n",
      "val_epoch: 91/300 Step: 292 loss : 1.1634 accuracy: 0.4110: 100%|██████████| 73/73 [00:23<00:00,  3.08it/s]\n",
      "epoch: 92/300 Step: 292 loss : 1.0813 accuracy: 0.4046: 100%|██████████| 291/291 [09:50<00:00,  2.03s/it]\n",
      "val_epoch: 92/300 Step: 292 loss : 1.1492 accuracy: 0.4041: 100%|██████████| 73/73 [00:23<00:00,  3.08it/s]\n",
      "epoch: 93/300 Step: 103 loss : 1.0745 accuracy: 0.3775:  35%|███▌      | 102/291 [03:29<06:27,  2.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m cost1\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# cost에 대한 backward 구함\u001b[39;00m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39msecond_step(zero_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m acc_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39macc\n\u001b[1;32m     38\u001b[0m train\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m300\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39mcount\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_loss\u001b[38;5;241m/\u001b[39mcount\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "d = datetime.datetime.now()\n",
    "now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "print(f'[deeplearning Start]')\n",
    "print(f'deeplearning Start Time : {now_time}')\n",
    "MIN_loss=5000\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "sig=nn.Sigmoid()\n",
    "val_acc_list=[]\n",
    "MIN_acc=0\n",
    "check_val=1000\n",
    "\n",
    "for epoch in range(300):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    model.train()\n",
    "    for x, y in train:\n",
    "        y = y.float().to(device)\n",
    "        count+=1\n",
    "        x=x.float().to(device)\n",
    "        enable_running_stats(model)\n",
    "        predict = model(x).to(device)\n",
    "        cost = criterion(predict.softmax(dim=1), y.argmax(dim=1)) # cost 구함\n",
    "        acc=accuracy(predict.softmax(dim=1), y.argmax(dim=1))\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        disable_running_stats(model)\n",
    "        predict = model(x).to(device)\n",
    "        cost1 = criterion(predict.softmax(dim=1), y.argmax(dim=1)) # cost 구함\n",
    "        cost1.backward() # cost에 대한 backward 구함\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        running_loss += cost.item()\n",
    "        acc_loss+=acc\n",
    "        train.set_description(f\"epoch: {epoch+1}/{300} Step: {count+1} loss : {running_loss/count:.4f} accuracy: {acc_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "    train_acc_list.append((acc_loss/count).cpu().detach().numpy())  \n",
    "    val=tqdm(test_dataloader)\n",
    "    path_list=[]\n",
    "    model.eval()\n",
    "    val_count=0\n",
    "    val_running_loss = 0.0\n",
    "    val_acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        total_y = torch.zeros((len(test_dataloader)*batch_size+batch_size, 3)).to(device)\n",
    "        total_prob = torch.zeros((len(test_dataloader)*batch_size+batch_size, 3)).to(device)\n",
    "        for x,y in val:\n",
    "            y = y.to(device).float()\n",
    "            val_count+=1\n",
    "            x=x.to(device).float()\n",
    "            predict = model(x).to(device)\n",
    "            cost = criterion(predict.softmax(dim=1), y.argmax(dim=1)) # cost 구함\n",
    "            acc=accuracy(predict.softmax(dim=1), y.argmax(dim=1))\n",
    "            total_y[val_count*batch_size:val_count*batch_size+batch_size] = y.squeeze(dim=1)\n",
    "            total_prob[val_count*batch_size:val_count*batch_size+batch_size] = predict.softmax(dim=1)\n",
    "            val_running_loss+=cost.item()\n",
    "            val_acc_loss+=acc\n",
    "            val.set_description(f\"val_epoch: {epoch+1}/{300} Step: {count+1} loss : {val_running_loss/val_count:.4f} accuracy: {val_acc_loss/val_count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/val_count))\n",
    "        val_acc_list.append((val_acc_loss/val_count).cpu().detach().numpy())  \n",
    "        f1 = f1_score(total_y.cpu().argmax(axis=1),total_prob.cpu().argmax(axis=1), average='micro')\n",
    "        torch.save(model.state_dict(), '../../model/mteg/videomae/1-fold/f1_'+str(round(f1,4))+'.pt')\n",
    "end = time.time()\n",
    "d = datetime.datetime.now()\n",
    "now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "print(f'deeplearning Time : {now_time}s Time taken : {start-end}')\n",
    "print(f'[deeplearning End]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
