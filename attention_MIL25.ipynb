{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import os\n",
    "import torchmetrics\n",
    "import datetime\n",
    "import timm\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import datetime\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=4\n",
    "image_count=50\n",
    "img_size=256\n",
    "tf = ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2423/2423 [04:47<00:00,  8.43it/s]\n",
      "100%|██████████| 300/300 [00:33<00:00,  8.86it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_image_path='../../data/frame/train/*'\n",
    "test_image_path='../../data/frame/val/*'\n",
    "\n",
    "val_image_path='../../data/frame/test/*'\n",
    "\n",
    "label_data=pd.read_csv('../../data/label_data.csv',encoding='cp949')  \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "\n",
    "        self.label = label_list\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor = self.img_path[idx]\n",
    "    \n",
    "        label_tensor =  self.label[idx]\n",
    "        return image_tensor, label_tensor\n",
    "    \n",
    "train_image_list=glob(train_image_path)\n",
    "train_label_list=[]\n",
    "train_image_tensor = torch.empty((len(train_image_list),image_count,3, img_size, img_size))\n",
    "for i in range(len(train_image_list)):\n",
    "    folder_name=os.path.basename(train_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    train_label_list.append(label) \n",
    "    image_file_list = glob(train_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "\n",
    "\n",
    "test_image_list=glob(test_image_path)\n",
    "test_label_list=[]\n",
    "test_image_tensor = torch.empty((len(test_image_list),image_count,3, img_size, img_size))\n",
    "for i in range(len(test_image_list)):\n",
    "    folder_name=os.path.basename(test_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    test_label_list.append(label) \n",
    "    image_file_list = glob(test_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "            \n",
    "val_image_list=glob(val_image_path)\n",
    "val_label_list=[]\n",
    "val_image_tensor = torch.empty((len(val_image_list),image_count,3, img_size, img_size))\n",
    "for i in range(len(val_image_list)):\n",
    "    folder_name=os.path.basename(val_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    val_label_list.append(label) \n",
    "    image_file_list = glob(val_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            val_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            val_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            val_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "            \n",
    "train_dataset = CustomDataset(train_image_tensor, F.one_hot(torch.tensor(train_label_list).to(torch.int64)))\n",
    "val_dataset = CustomDataset(val_image_tensor, F.one_hot(torch.tensor(val_label_list).to(torch.int64)))\n",
    "test_dataset = CustomDataset(test_image_tensor, F.one_hot(torch.tensor(test_label_list).to(torch.int64)))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "AttentionMILModel                                       [4, 3]                    --\n",
       "├─FeatureExtractor: 1-1                                 [200, 1408]               --\n",
       "│    └─Sequential: 2-1                                  [200, 1408]               --\n",
       "│    │    └─Conv2d: 3-1                                 [200, 32, 128, 128]       864\n",
       "│    │    └─BatchNormAct2d: 3-2                         [200, 32, 128, 128]       64\n",
       "│    │    └─Sequential: 3-3                             [200, 352, 8, 8]          7,201,634\n",
       "│    │    └─Conv2d: 3-4                                 [200, 1408, 8, 8]         495,616\n",
       "│    │    └─BatchNormAct2d: 3-5                         [200, 1408, 8, 8]         2,816\n",
       "│    │    └─SelectAdaptivePool2d: 3-6                   [200, 1408]               --\n",
       "├─Sequential: 1-2                                       [4, 50, 1]                --\n",
       "│    └─Linear: 2-2                                      [4, 50, 128]              180,352\n",
       "│    └─Tanh: 2-3                                        [4, 50, 128]              --\n",
       "│    └─Linear: 2-4                                      [4, 50, 1]                129\n",
       "├─Dropout: 1-3                                          [4, 1408]                 --\n",
       "├─Linear: 1-4                                           [4, 3]                    4,227\n",
       "=========================================================================================================\n",
       "Total params: 7,885,702\n",
       "Trainable params: 7,885,702\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 171.69\n",
       "=========================================================================================================\n",
       "Input size (MB): 157.29\n",
       "Forward/backward pass size (MB): 20488.48\n",
       "Params size (MB): 31.27\n",
       "Estimated Total Size (MB): 20677.04\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model('efficientnet_b2', pretrained=True)\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "class AttentionMILModel(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(AttentionMILModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "        self.dropout=torch.nn.Dropout(0.2)\n",
    "    def forward(self, inputs):\n",
    "        batch_size, num_tiles, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Flatten the inputs\n",
    "        inputs = inputs.view(-1, channels, height, width)\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size * num_tiles, 2048, 1, 1)\n",
    "        \n",
    "        # Reshape features\n",
    "        features = features.view(batch_size, num_tiles, -1)  # Shape: (batch_size, num_tiles, 2048)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(features)  # Shape: (batch_size, num_tiles, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)  # Normalize attention weights\n",
    "        \n",
    "        # Apply attention weights to features\n",
    "        attended_features = torch.sum(features * attention_weights, dim=1)  # Shape: (batch_size, 2048)\n",
    "        attended_features=self.dropout(attended_features)\n",
    "        attended_features=F.relu(attended_features)\n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(attended_features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "Feature_Extractor=FeatureExtractor()\n",
    "model = AttentionMILModel(3,1408,Feature_Extractor)\n",
    "model = model.to(device)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "summary(model,(batch_size,image_count,3,img_size,img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deeplearning Start]\n",
      "deeplearning Start Time : 2023-10-26 17:58:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.30it/s]\n",
      "Validation epoch: 1/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.14it/s]\n",
      "epoch: 2/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.31it/s]\n",
      "Validation epoch: 2/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.05it/s]\n",
      "epoch: 3/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.30it/s]\n",
      "Validation epoch: 3/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.03it/s]\n",
      "epoch: 4/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 4/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.00it/s]\n",
      "epoch: 5/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.29it/s]\n",
      "Validation epoch: 5/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.03it/s]\n",
      "epoch: 6/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.29it/s]\n",
      "Validation epoch: 6/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.25it/s]\n",
      "epoch: 7/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 7/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.24it/s]\n",
      "epoch: 8/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.31it/s]\n",
      "Validation epoch: 8/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.15it/s]\n",
      "epoch: 9/50 Step: 606: 100%|██████████| 605/605 [04:20<00:00,  2.32it/s]\n",
      "Validation epoch: 9/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.07it/s]\n",
      "epoch: 10/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.31it/s]\n",
      "Validation epoch: 10/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.37it/s]\n",
      "epoch: 11/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.30it/s]\n",
      "Validation epoch: 11/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.12it/s]\n",
      "epoch: 12/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.31it/s]\n",
      "Validation epoch: 12/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.31it/s]\n",
      "epoch: 13/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.31it/s]\n",
      "Validation epoch: 13/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.18it/s]\n",
      "epoch: 14/50 Step: 606: 100%|██████████| 605/605 [04:25<00:00,  2.28it/s]\n",
      "Validation epoch: 14/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.96it/s]\n",
      "epoch: 15/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 15/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.20it/s]\n",
      "epoch: 16/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.30it/s]\n",
      "Validation epoch: 16/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.13it/s]\n",
      "epoch: 17/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.31it/s]\n",
      "Validation epoch: 17/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.20it/s]\n",
      "epoch: 18/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.30it/s]\n",
      "Validation epoch: 18/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.99it/s]\n",
      "epoch: 19/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.31it/s]\n",
      "Validation epoch: 19/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.04it/s]\n",
      "epoch: 20/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 20/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.91it/s]\n",
      "epoch: 21/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 21/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.07it/s]\n",
      "epoch: 22/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 22/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.10it/s]\n",
      "epoch: 23/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.30it/s]\n",
      "Validation epoch: 23/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.02it/s]\n",
      "epoch: 24/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.30it/s]\n",
      "Validation epoch: 24/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.01it/s]\n",
      "epoch: 25/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 25/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.00it/s]\n",
      "epoch: 26/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.30it/s]\n",
      "Validation epoch: 26/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.17it/s]\n",
      "epoch: 27/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 27/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.13it/s]\n",
      "epoch: 28/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 28/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.90it/s]\n",
      "epoch: 29/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.29it/s]\n",
      "Validation epoch: 29/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.13it/s]\n",
      "epoch: 30/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.28it/s]\n",
      "Validation epoch: 30/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.16it/s]\n",
      "epoch: 31/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.30it/s]\n",
      "Validation epoch: 31/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.93it/s]\n",
      "epoch: 32/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 32/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.92it/s]\n",
      "epoch: 33/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 33/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.92it/s]\n",
      "epoch: 34/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.29it/s]\n",
      "Validation epoch: 34/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.96it/s]\n",
      "epoch: 35/50 Step: 606: 100%|██████████| 605/605 [04:25<00:00,  2.28it/s]\n",
      "Validation epoch: 35/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.00it/s]\n",
      "epoch: 36/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.28it/s]\n",
      "Validation epoch: 36/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.87it/s]\n",
      "epoch: 37/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 37/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.90it/s]\n",
      "epoch: 38/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.30it/s]\n",
      "Validation epoch: 38/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.06it/s]\n",
      "epoch: 39/50 Step: 606: 100%|██████████| 605/605 [04:22<00:00,  2.30it/s]\n",
      "Validation epoch: 39/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.96it/s]\n",
      "epoch: 40/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 40/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.99it/s]\n",
      "epoch: 41/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 41/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.91it/s]\n",
      "epoch: 42/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 42/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.98it/s]\n",
      "epoch: 43/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.30it/s]\n",
      "Validation epoch: 43/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.85it/s]\n",
      "epoch: 44/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.30it/s]\n",
      "Validation epoch: 44/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.13it/s]\n",
      "epoch: 45/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 45/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.94it/s]\n",
      "epoch: 46/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.28it/s]\n",
      "Validation epoch: 46/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.98it/s]\n",
      "epoch: 47/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 47/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.94it/s]\n",
      "epoch: 48/50 Step: 606: 100%|██████████| 605/605 [04:25<00:00,  2.28it/s]\n",
      "Validation epoch: 48/50 Step: 76: 100%|██████████| 75/75 [00:11<00:00,  6.79it/s]\n",
      "epoch: 49/50 Step: 606: 100%|██████████| 605/605 [04:24<00:00,  2.29it/s]\n",
      "Validation epoch: 49/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  6.91it/s]\n",
      "epoch: 50/50 Step: 606: 100%|██████████| 605/605 [04:23<00:00,  2.29it/s]\n",
      "Validation epoch: 50/50 Step: 76: 100%|██████████| 75/75 [00:10<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Time : 2023-10-26 21:47:27s Time taken : -13723.562340021133\n",
      "[deeplearning End]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "d = datetime.datetime.now()\n",
    "now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "print(f'[deeplearning Start]')\n",
    "print(f'deeplearning Start Time : {now_time}')\n",
    "MIN_loss=5000\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "sig=nn.Sigmoid()\n",
    "val_acc_list=[]\n",
    "MIN_acc=0\n",
    "numSample_list = [len(image_Oropharynx_list),len(image_Tonguebase_list),len(image_Epiglottis_list)]\n",
    "weights = torch.Tensor([1 - (x / sum(numSample_list)) for x in numSample_list]).to(device)\n",
    "for epoch in range(50):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    model.train()\n",
    "    for x, y in train:\n",
    "        \n",
    "        y = y.to(device).float()\n",
    "        count+=1\n",
    "        x=x.to(device).float()\n",
    "        optimizer.zero_grad()  # optimizer zero 로 초기화\n",
    "        predict = model(x).to(device)\n",
    "        cost = F.cross_entropy(predict.softmax(dim=1), y,weight=weights) # cost 구함\n",
    "        acc=accuracy(predict.softmax(dim=1).argmax(dim=1),y.argmax(dim=1))\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.step() \n",
    "        running_loss += cost.item()\n",
    "        acc_loss+=acc\n",
    "        train.set_description(f\"epoch: {epoch+1}/{50} Step: {count+1} loss : {running_loss/count:.4f} accuracy: {acc_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "    train_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "#validation\n",
    "    val=tqdm(validation_dataloader)\n",
    "    model.eval()\n",
    "    count=0\n",
    "    val_running_loss=0.0\n",
    "    acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val:\n",
    "            y = y.to(device).float()\n",
    "            count+=1\n",
    "            x=x.to(device).float()\n",
    "            predict = model(x).to(device)\n",
    "            cost = F.cross_entropy(predict.softmax(dim=1), y,weight=weights) # cost 구함\n",
    "            acc=accuracy(predict.softmax(dim=1).argmax(dim=1),y.argmax(dim=1))\n",
    "            val_running_loss+=cost.item()\n",
    "            acc_loss+=acc\n",
    "            val.set_description(f\"Validation epoch: {epoch+1}/{50} Step: {count+1} loss : {val_running_loss/count:.4f}  accuracy: {acc_loss/count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/count))\n",
    "        val_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "\n",
    "        \n",
    "        \n",
    "    if MIN_loss>(val_running_loss/count):\n",
    "        torch.save(model.state_dict(), '../../model/attention_123_loss.pt')\n",
    "        MIN_loss=(val_running_loss/count)\n",
    "        \n",
    "    if MIN_acc<(acc_loss/count):\n",
    "        torch.save(model.state_dict(), '../../model/attention_123_acc.pt')\n",
    "        MIN_acc=(acc_loss/count)\n",
    "        \n",
    "torch.save(model.state_dict(), '../../model/attention_1_eff50_MIL.pt')\n",
    "end = time.time()\n",
    "d = datetime.datetime.now()\n",
    "now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "print(f'deeplearning Time : {now_time}s Time taken : {start-end}')\n",
    "print(f'[deeplearning End]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
