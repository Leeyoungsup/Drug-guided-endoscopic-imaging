{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "import json\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# 경고 메시지를 무시하도록 설정\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# 경고 메시지를 무시하도록 설정\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result\n",
    "\n",
    "\n",
    "def Preprocessing(file_list,dataset_calss, label_data):\n",
    "    start = time.time()\n",
    "    d = datetime.datetime.now()\n",
    "    now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "    print(f'[Preprocessing Start]')\n",
    "    print(f'Preprocessing Start Time : {now_time}')\n",
    "    frame_path = '../../data/valum/'+dataset_calss+'/'    \n",
    "    for i in tqdm(range(len(file_list))):\n",
    "       \n",
    "        count = 0\n",
    "        vidcap = torchvision.io.read_video(file_list[i])\n",
    "        fps = int(vidcap[2]['video_fps'])\n",
    "        video = np.array(vidcap[0], dtype=np.uint8)\n",
    "        video_crop = np.zeros(\n",
    "            (len(video)-1, video.shape[1], video.shape[2], 3))\n",
    "        for j in range(len(video_crop)):\n",
    "            video_crop[j] = video[j+1]-video[j]\n",
    "        video_crop = video_crop.sum(axis=0)\n",
    "        video_crop = video_crop.sum(axis=2)\n",
    "        video_crop = ((video_crop/video_crop.max())*255).astype(np.uint8)\n",
    "        y1 = np.where(video_crop > 200)[0].min()\n",
    "        y2 = np.where(video_crop > 200)[0].max()\n",
    "        x1 = np.where(video_crop > 200)[1].min()\n",
    "        x2 = np.where(video_crop > 200)[1].max()\n",
    "        video_name = os.path.basename(file_list[i])\n",
    "        dst_label = label_data.loc[label_data[\"File Name\"] == video_name]\n",
    "        wake = str(dst_label['구분값'].item())\n",
    "        Serial_Number = str(dst_label['일련번호'].item())\n",
    "        file_name = Serial_Number+wake\n",
    "        createDirectory(frame_path+file_name)\n",
    "        for k in range(0, len(video), fps//5):\n",
    "            img = Image.fromarray(video[k, y1:y2, x1:x2])\n",
    "            img.resize((256, 256)).save(\n",
    "                frame_path+file_name+\"/%06d.jpg\" % count)\n",
    "            count += 1\n",
    "    \n",
    "    end = time.time()\n",
    "    d = datetime.datetime.now()\n",
    "    now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "    print(f'Preprocessing Time : {now_time}s Time taken : {end-start}')\n",
    "    print(f'[Preprocessing End]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2864bbb5b1b04d74a38a59cb4387f2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd4a6d8949e470b8a57c761d250d800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ce801f136e48d1be6d47af1dc108ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_data = pd.DataFrame(columns=['구분값', '일련번호', 'velum','File Name'])\n",
    "train_label_data = pd.DataFrame(columns=['구분값', '일련번호', 'velum','File Name'])\n",
    "test_label_data = pd.DataFrame(columns=['구분값', '일련번호', 'velum','File Name'])\n",
    "val_label_data = pd.DataFrame(columns=['구분값', '일련번호', 'velum','File Name'])\n",
    "classes = ['no', 'partial', 'complete']\n",
    "label_list=glob('../../../../YS_Baik/5.NIA_42/02_2_classification_gachon/Velum/01_data/train/*.json')\n",
    "train_count=0\n",
    "test_count=0\n",
    "val_count=0\n",
    "for i in tqdm(range(len(label_list))):\n",
    "    with open(label_list[i], 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    try:\n",
    "        separation_value= json_data['videos']['id'][json_data['videos']['id'].find('_')-1]\n",
    "        serial_number=json_data['videos']['id'][:json_data['videos']['id'].find('_')-1]\n",
    "        causes_of_velum=json_data['videos']['id']\n",
    "        label_data.loc[train_count]=[int(separation_value),int(serial_number),classes.index(json_data['metas']['obstruction'])+1,json_data['videos']['filename']]\n",
    "        train_label_data.loc[train_count]=[int(separation_value),int(serial_number),classes.index(json_data['metas']['obstruction'])+1,json_data['videos']['filename']]\n",
    "        train_count+=1\n",
    "    except:\n",
    "        print(json_data['videos']['id'])\n",
    "train_label_data.to_csv('../../data/valum/train_label.csv',index=False)\n",
    "label_list=glob('../../../../YS_Baik/5.NIA_42/02_2_classification_gachon/Velum/01_data/test/*.json')\n",
    "for i in tqdm(range(len(label_list))):\n",
    "    with open(label_list[i], 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    try:\n",
    "        separation_value= json_data['videos']['id'][json_data['videos']['id'].find('_')-1]\n",
    "        serial_number=json_data['videos']['id'][:json_data['videos']['id'].find('_')-1]\n",
    "        causes_of_velum=json_data['videos']['id']\n",
    "        label_data.loc[train_count+test_count]=[int(separation_value),int(serial_number),classes.index(json_data['metas']['obstruction'])+1,json_data['videos']['filename']]\n",
    "        test_label_data.loc[test_count]=[int(separation_value),int(serial_number),classes.index(json_data['metas']['obstruction'])+1,json_data['videos']['filename']]\n",
    "        test_count+=1\n",
    "    except:\n",
    "        print(json_data['videos']['id'])\n",
    "test_label_data.to_csv('../../data/valum/test_label.csv',index=False)\n",
    "label_list=glob('../../../../YS_Baik/5.NIA_42/02_2_classification_gachon/Velum/01_data/val/*.json')\n",
    "for i in tqdm(range(len(label_list))):\n",
    "    with open(label_list[i], 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    f.close()\n",
    "   \n",
    "    try:\n",
    "        separation_value= json_data['videos']['id'][json_data['videos']['id'].find('_')-1]\n",
    "        serial_number=json_data['videos']['id'][:json_data['videos']['id'].find('_')-1]\n",
    "        causes_of_velum=json_data['videos']['id']\n",
    "        label_data.loc[train_count+test_count+val_count]=[int(separation_value),int(serial_number),classes.index(json_data['metas']['obstruction'])+1,json_data['videos']['filename']]\n",
    "        val_label_data.loc[val_count]=[int(separation_value),int(serial_number),classes.index(json_data['metas']['obstruction'])+1,json_data['videos']['filename']]\n",
    "        val_count+=1\n",
    "    except:\n",
    "        print(json_data['videos']['id'])\n",
    "val_label_data.to_csv('../../data/valum/val_label.csv',index=False)\n",
    "label_data.to_csv('../../data/valum/label.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = pd.read_csv('../../data/valum/label.csv')\n",
    "train_label_data = pd.read_csv('../../data/valum/train_label.csv')\n",
    "test_label_data = pd.read_csv('../../data/valum/test_label.csv')\n",
    "val_label_data = pd.read_csv('../../data/valum/val_label.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Preprocessing Start]\n",
      "Preprocessing Start Time : 2024-6-3 14:16:31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8c91b9426944d6ae86d1a23ff5e004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7f7b6af260d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/tqdm/std.py\", line 1149, in __del__\n",
      "    self.close()\n",
      "  File \"/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/tqdm/notebook.py\", line 281, in close\n",
      "    self.disp(bar_style='success', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Time : 2024-6-3 18:8:17s Time taken : 13905.776525259018\n",
      "[Preprocessing End]\n",
      "[Preprocessing Start]\n",
      "Preprocessing Start Time : 2024-6-3 18:8:17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3244e3c1f2842f09d68e5081cd9cf5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Time : 2024-6-3 18:41:11s Time taken : 1973.983473777771\n",
      "[Preprocessing End]\n",
      "[Preprocessing Start]\n",
      "Preprocessing Start Time : 2024-6-3 18:41:11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c2dac32a924b459c1e624282a577e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Time : 2024-6-3 19:33:45s Time taken : 3153.7880914211273\n",
      "[Preprocessing End]\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "for i in range(len(train_label_data)):\n",
    "    file_list.append('../../../../YS_Baik/5.NIA_42/02_2_classification_gachon/Velum/01_data/train/'+train_label_data['File Name'][i])\n",
    "Preprocessing(file_list,'train', train_label_data)\n",
    "file_list = []\n",
    "for i in range(len(test_label_data)):\n",
    "    file_list.append('../../../../YS_Baik/5.NIA_42/02_2_classification_gachon/Velum/01_data/test/'+test_label_data['File Name'][i])\n",
    "Preprocessing(file_list,'test', test_label_data)\n",
    "file_list = []\n",
    "for i in range(len(val_label_data)):\n",
    "    file_list.append('../../../../YS_Baik/5.NIA_42/02_2_classification_gachon/Velum/01_data/val/'+val_label_data['File Name'][i])\n",
    "Preprocessing(file_list,'val', val_label_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
