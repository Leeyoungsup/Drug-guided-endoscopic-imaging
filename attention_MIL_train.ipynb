{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import os\n",
    "import torchmetrics\n",
    "import timm\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=4\n",
    "image_count=50\n",
    "img_size=256\n",
    "tf = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1035/1035 [02:04<00:00,  8.32it/s]\n",
      "100%|██████████| 134/134 [00:16<00:00,  8.07it/s]\n",
      "100%|██████████| 133/133 [00:13<00:00, 10.21it/s]\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, id,image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "\n",
    "        self.label = label_list\n",
    "        self.id=id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_tensor=self.id[idx]\n",
    "        image_tensor = self.img_path[idx]\n",
    "    \n",
    "        label_tensor =  self.label[idx]\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "\n",
    "train_data=pd.read_csv('../../data/mteg_data/supine_frame/train_label.csv') \n",
    "file_path='../../data/mteg_data/supine_frame/train/'\n",
    "train_image_list=[]\n",
    "for i in range(len(train_data)):\n",
    "    file_name=train_data.loc[i]['File Name']\n",
    "    id=file_name[:file_name.find('_')]\n",
    "    train_image_list.append(file_path+id)\n",
    "label_data=pd.read_csv('../../data/mteg_data/supine_frame/label.csv')  \n",
    "train_label_list=[]\n",
    "train_id_list=[]\n",
    "train_image_tensor = torch.empty((len(train_image_list),image_count,3, img_size, img_size))\n",
    "for i in tqdm(range(len(train_image_list))):\n",
    "    folder_name=os.path.basename(train_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    train_id_list.append(folder_name)\n",
    "    train_label_list.append(label-1) \n",
    "    image_file_list = glob(train_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "            \n",
    "val_data=pd.read_csv('../../data/mteg_data/supine_frame/val_label.csv') \n",
    "file_path='../../data/mteg_data/supine_frame/val/'\n",
    "val_image_list=[]\n",
    "for i in range(len(val_data)):\n",
    "    file_name=val_data.loc[i]['File Name']\n",
    "    id=file_name[:file_name.find('_')]\n",
    "    val_image_list.append(file_path+id)\n",
    "label_data=pd.read_csv('../../data/mteg_data/supine_frame/label.csv')  \n",
    "val_label_list=[]\n",
    "val_id_list=[]\n",
    "val_image_tensor = torch.empty((len(val_image_list),image_count,3, img_size, img_size))\n",
    "for i in tqdm(range(len(val_image_list))):\n",
    "    folder_name=os.path.basename(val_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    val_id_list.append(folder_name)\n",
    "    val_label_list.append(label-1) \n",
    "    image_file_list = glob(val_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            val_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            val_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            val_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "\n",
    "test_data=pd.read_csv('../../data/mteg_data/supine_frame/test_label.csv') \n",
    "file_path='../../data/mteg_data/supine_frame/test/'\n",
    "test_image_list=[]\n",
    "for i in range(len(test_data)):\n",
    "    file_name=test_data.loc[i]['File Name']\n",
    "    id=file_name[:file_name.find('_')]\n",
    "    test_image_list.append(file_path+id)\n",
    "label_data=pd.read_csv('../../data/mteg_data/supine_frame/label.csv')  \n",
    "test_label_list=[]\n",
    "test_id_list=[]\n",
    "test_image_tensor = torch.empty((len(test_image_list),image_count,3, img_size, img_size))\n",
    "for i in tqdm(range(len(test_image_list))):\n",
    "    folder_name=os.path.basename(test_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    test_id_list.append(folder_name)\n",
    "    test_label_list.append(label-1) \n",
    "    image_file_list = glob(test_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "\n",
    "train_id_list.extend(val_id_list)\n",
    "train_image_tensor = torch.concat((train_image_tensor,val_image_tensor))\n",
    "train_label_list.extend(val_label_list)\n",
    "train_dataset = CustomDataset(train_id_list,train_image_tensor, F.one_hot(torch.tensor(train_label_list).to(torch.int64)))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataset = CustomDataset(test_id_list,test_image_tensor, F.one_hot(torch.tensor(test_label_list).to(torch.int64)))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "AttentionMILModel                                       [4, 3]                    --\n",
       "├─FeatureExtractor: 1-1                                 [200, 1408]               --\n",
       "│    └─Sequential: 2-1                                  [200, 1408]               --\n",
       "│    │    └─Conv2d: 3-1                                 [200, 32, 128, 128]       864\n",
       "│    │    └─BatchNormAct2d: 3-2                         [200, 32, 128, 128]       64\n",
       "│    │    └─Sequential: 3-3                             [200, 352, 8, 8]          7,201,634\n",
       "│    │    └─Conv2d: 3-4                                 [200, 1408, 8, 8]         495,616\n",
       "│    │    └─BatchNormAct2d: 3-5                         [200, 1408, 8, 8]         2,816\n",
       "│    │    └─SelectAdaptivePool2d: 3-6                   [200, 1408]               --\n",
       "├─Sequential: 1-2                                       [4, 50, 1]                --\n",
       "│    └─Linear: 2-2                                      [4, 50, 128]              180,352\n",
       "│    └─Tanh: 2-3                                        [4, 50, 128]              --\n",
       "│    └─Linear: 2-4                                      [4, 50, 1]                129\n",
       "├─Dropout: 1-3                                          [4, 1408]                 --\n",
       "├─Linear: 1-4                                           [4, 3]                    4,227\n",
       "=========================================================================================================\n",
       "Total params: 7,885,702\n",
       "Trainable params: 7,885,702\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 171.69\n",
       "=========================================================================================================\n",
       "Input size (MB): 157.29\n",
       "Forward/backward pass size (MB): 20488.48\n",
       "Params size (MB): 31.27\n",
       "Estimated Total Size (MB): 20677.04\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model('efficientnet_b2', pretrained=True)\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "class AttentionMILModel(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(AttentionMILModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "        self.dropout=torch.nn.Dropout(0.2)\n",
    "    def forward(self, inputs):\n",
    "        batch_size, num_tiles, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Flatten the inputs\n",
    "        inputs = inputs.view(-1, channels, height, width)\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size * num_tiles, 2048, 1, 1)\n",
    "        \n",
    "        # Reshape features\n",
    "        features = features.view(batch_size, num_tiles, -1)  # Shape: (batch_size, num_tiles, 2048)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(features)  # Shape: (batch_size, num_tiles, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)  # Normalize attention weights\n",
    "        \n",
    "        # Apply attention weights to features\n",
    "        attended_features = torch.sum(features * attention_weights, dim=1)  # Shape: (batch_size, 2048)\n",
    "        attended_features=self.dropout(attended_features)\n",
    "        attended_features=F.relu(attended_features)\n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(attended_features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups\n",
    "        \n",
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, _BatchNorm):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "            \n",
    "class_counts = [train_label_list.count(0), train_label_list.count(1), train_label_list.count(2)]\n",
    "class_weights = torch.tensor([sum(class_counts) / c for c in class_counts], dtype=torch.float)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "Feature_Extractor=FeatureExtractor()\n",
    "model = AttentionMILModel(3,1408,Feature_Extractor)\n",
    "model = model.to(device)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "base_optimizer = torch.optim.SGD\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=2e-4, momentum=0.9)\n",
    "summary(model,(batch_size,image_count,3,img_size,img_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deeplearning Start]\n",
      "deeplearning Start Time : 2024-4-29 14:6:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/300 Step: 293 loss : 1.2199 accuracy: 0.3647: 100%|██████████| 292/292 [03:57<00:00,  1.23it/s]\n",
      "val_epoch: 1/300 Step: 293 loss : 1.0929 accuracy: 0.4773: 100%|██████████| 33/33 [00:04<00:00,  7.46it/s]\n",
      "epoch: 2/300 Step: 293 loss : 1.2025 accuracy: 0.4204: 100%|██████████| 292/292 [03:57<00:00,  1.23it/s]\n",
      "val_epoch: 2/300 Step: 293 loss : 1.0966 accuracy: 0.4318: 100%|██████████| 33/33 [00:04<00:00,  7.18it/s]\n",
      "epoch: 3/300 Step: 293 loss : 1.1985 accuracy: 0.4092: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 3/300 Step: 293 loss : 1.0901 accuracy: 0.4394: 100%|██████████| 33/33 [00:04<00:00,  7.20it/s]\n",
      "epoch: 4/300 Step: 293 loss : 1.1907 accuracy: 0.4521: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 4/300 Step: 293 loss : 1.0983 accuracy: 0.4167: 100%|██████████| 33/33 [00:04<00:00,  7.15it/s]\n",
      "epoch: 5/300 Step: 293 loss : 1.1818 accuracy: 0.4743: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 5/300 Step: 293 loss : 1.0983 accuracy: 0.4318: 100%|██████████| 33/33 [00:04<00:00,  7.08it/s]\n",
      "epoch: 6/300 Step: 293 loss : 1.1840 accuracy: 0.4555: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 6/300 Step: 293 loss : 1.0933 accuracy: 0.4242: 100%|██████████| 33/33 [00:04<00:00,  7.24it/s]\n",
      "epoch: 7/300 Step: 293 loss : 1.1810 accuracy: 0.4692: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 7/300 Step: 293 loss : 1.0881 accuracy: 0.4773: 100%|██████████| 33/33 [00:04<00:00,  7.15it/s]\n",
      "epoch: 8/300 Step: 293 loss : 1.1703 accuracy: 0.4889: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 8/300 Step: 293 loss : 1.0937 accuracy: 0.4091: 100%|██████████| 33/33 [00:04<00:00,  7.24it/s]\n",
      "epoch: 9/300 Step: 293 loss : 1.1700 accuracy: 0.4726: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 9/300 Step: 293 loss : 1.0927 accuracy: 0.4091: 100%|██████████| 33/33 [00:04<00:00,  7.16it/s]\n",
      "epoch: 10/300 Step: 293 loss : 1.1610 accuracy: 0.4923: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 10/300 Step: 293 loss : 1.0880 accuracy: 0.4318: 100%|██████████| 33/33 [00:04<00:00,  7.12it/s]\n",
      "epoch: 11/300 Step: 293 loss : 1.1650 accuracy: 0.5009: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 11/300 Step: 293 loss : 1.0932 accuracy: 0.4015: 100%|██████████| 33/33 [00:04<00:00,  6.99it/s]\n",
      "epoch: 12/300 Step: 293 loss : 1.1585 accuracy: 0.5000: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 12/300 Step: 293 loss : 1.0923 accuracy: 0.4091: 100%|██████████| 33/33 [00:04<00:00,  7.24it/s]\n",
      "epoch: 13/300 Step: 293 loss : 1.1595 accuracy: 0.4940: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 13/300 Step: 293 loss : 1.0879 accuracy: 0.4318: 100%|██████████| 33/33 [00:04<00:00,  7.17it/s]\n",
      "epoch: 14/300 Step: 293 loss : 1.1561 accuracy: 0.5060: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 14/300 Step: 293 loss : 1.0818 accuracy: 0.4848: 100%|██████████| 33/33 [00:04<00:00,  7.27it/s]\n",
      "epoch: 15/300 Step: 293 loss : 1.1534 accuracy: 0.5077: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 15/300 Step: 293 loss : 1.0761 accuracy: 0.5076: 100%|██████████| 33/33 [00:04<00:00,  7.16it/s]\n",
      "epoch: 16/300 Step: 293 loss : 1.1473 accuracy: 0.5077: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 16/300 Step: 293 loss : 1.0791 accuracy: 0.4848: 100%|██████████| 33/33 [00:04<00:00,  7.36it/s]\n",
      "epoch: 17/300 Step: 293 loss : 1.1477 accuracy: 0.5017: 100%|██████████| 292/292 [03:58<00:00,  1.23it/s]\n",
      "val_epoch: 17/300 Step: 293 loss : 1.0806 accuracy: 0.4470: 100%|██████████| 33/33 [00:04<00:00,  7.21it/s]\n",
      "epoch: 18/300 Step: 293 loss : 1.1442 accuracy: 0.5146: 100%|██████████| 292/292 [03:57<00:00,  1.23it/s]\n",
      "val_epoch: 18/300 Step: 293 loss : 1.0754 accuracy: 0.5000: 100%|██████████| 33/33 [00:04<00:00,  7.35it/s]\n",
      "epoch: 19/300 Step: 293 loss : 1.1364 accuracy: 0.5128: 100%|██████████| 292/292 [03:57<00:00,  1.23it/s]\n",
      "val_epoch: 19/300 Step: 293 loss : 1.0726 accuracy: 0.5000: 100%|██████████| 33/33 [00:04<00:00,  7.40it/s]\n",
      "epoch: 20/300 Step: 293 loss : 1.1285 accuracy: 0.5308: 100%|██████████| 292/292 [03:57<00:00,  1.23it/s]\n",
      "val_epoch: 20/300 Step: 293 loss : 1.0733 accuracy: 0.4621: 100%|██████████| 33/33 [00:04<00:00,  7.29it/s]\n",
      "epoch: 21/300 Step: 293 loss : 1.1287 accuracy: 0.5197: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 21/300 Step: 293 loss : 1.0743 accuracy: 0.4924: 100%|██████████| 33/33 [00:04<00:00,  7.34it/s]\n",
      "epoch: 22/300 Step: 293 loss : 1.1269 accuracy: 0.5325: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 22/300 Step: 293 loss : 1.0701 accuracy: 0.5000: 100%|██████████| 33/33 [00:04<00:00,  7.31it/s]\n",
      "epoch: 23/300 Step: 293 loss : 1.1241 accuracy: 0.5411: 100%|██████████| 292/292 [03:58<00:00,  1.23it/s]\n",
      "val_epoch: 23/300 Step: 293 loss : 1.0640 accuracy: 0.5379: 100%|██████████| 33/33 [00:04<00:00,  7.20it/s]\n",
      "epoch: 24/300 Step: 293 loss : 1.1190 accuracy: 0.5334: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 24/300 Step: 293 loss : 1.0627 accuracy: 0.5303: 100%|██████████| 33/33 [00:04<00:00,  6.74it/s]\n",
      "epoch: 25/300 Step: 293 loss : 1.1189 accuracy: 0.5505: 100%|██████████| 292/292 [04:00<00:00,  1.21it/s]\n",
      "val_epoch: 25/300 Step: 293 loss : 1.0648 accuracy: 0.5303: 100%|██████████| 33/33 [00:04<00:00,  7.11it/s]\n",
      "epoch: 26/300 Step: 293 loss : 1.1148 accuracy: 0.5531: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 26/300 Step: 293 loss : 1.0664 accuracy: 0.4924: 100%|██████████| 33/33 [00:04<00:00,  7.32it/s]\n",
      "epoch: 27/300 Step: 293 loss : 1.1109 accuracy: 0.5402: 100%|██████████| 292/292 [04:02<00:00,  1.21it/s]\n",
      "val_epoch: 27/300 Step: 293 loss : 1.0593 accuracy: 0.5076: 100%|██████████| 33/33 [00:04<00:00,  7.18it/s]\n",
      "epoch: 28/300 Step: 293 loss : 1.1120 accuracy: 0.5351: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 28/300 Step: 293 loss : 1.0559 accuracy: 0.5379: 100%|██████████| 33/33 [00:04<00:00,  6.81it/s]\n",
      "epoch: 29/300 Step: 293 loss : 1.1137 accuracy: 0.5351: 100%|██████████| 292/292 [04:00<00:00,  1.21it/s]\n",
      "val_epoch: 29/300 Step: 293 loss : 1.0600 accuracy: 0.5303: 100%|██████████| 33/33 [00:04<00:00,  6.74it/s]\n",
      "epoch: 30/300 Step: 293 loss : 1.1021 accuracy: 0.5557: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 30/300 Step: 293 loss : 1.0576 accuracy: 0.5455: 100%|██████████| 33/33 [00:04<00:00,  7.12it/s]\n",
      "epoch: 31/300 Step: 293 loss : 1.1024 accuracy: 0.5531: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 31/300 Step: 293 loss : 1.0521 accuracy: 0.5455: 100%|██████████| 33/33 [00:04<00:00,  6.67it/s]\n",
      "epoch: 32/300 Step: 293 loss : 1.0980 accuracy: 0.5659: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 32/300 Step: 293 loss : 1.0623 accuracy: 0.5152: 100%|██████████| 33/33 [00:04<00:00,  6.77it/s]\n",
      "epoch: 33/300 Step: 293 loss : 1.1040 accuracy: 0.5411: 100%|██████████| 292/292 [04:02<00:00,  1.20it/s]\n",
      "val_epoch: 33/300 Step: 293 loss : 1.0612 accuracy: 0.5000: 100%|██████████| 33/33 [00:04<00:00,  7.08it/s]\n",
      "epoch: 34/300 Step: 293 loss : 1.0951 accuracy: 0.5479: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 34/300 Step: 293 loss : 1.0579 accuracy: 0.5152: 100%|██████████| 33/33 [00:05<00:00,  6.34it/s]\n",
      "epoch: 35/300 Step: 293 loss : 1.0953 accuracy: 0.5531: 100%|██████████| 292/292 [04:04<00:00,  1.20it/s]\n",
      "val_epoch: 35/300 Step: 293 loss : 1.0616 accuracy: 0.4848: 100%|██████████| 33/33 [00:04<00:00,  6.87it/s]\n",
      "epoch: 36/300 Step: 293 loss : 1.0934 accuracy: 0.5385: 100%|██████████| 292/292 [04:06<00:00,  1.18it/s]\n",
      "val_epoch: 36/300 Step: 293 loss : 1.0597 accuracy: 0.5152: 100%|██████████| 33/33 [00:04<00:00,  6.94it/s]\n",
      "epoch: 37/300 Step: 293 loss : 1.1020 accuracy: 0.5437: 100%|██████████| 292/292 [04:04<00:00,  1.19it/s]\n",
      "val_epoch: 37/300 Step: 293 loss : 1.0575 accuracy: 0.5227: 100%|██████████| 33/33 [00:04<00:00,  7.17it/s]\n",
      "epoch: 38/300 Step: 293 loss : 1.0938 accuracy: 0.5462: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 38/300 Step: 293 loss : 1.0552 accuracy: 0.5076: 100%|██████████| 33/33 [00:04<00:00,  7.50it/s]\n",
      "epoch: 39/300 Step: 293 loss : 1.0813 accuracy: 0.5548: 100%|██████████| 292/292 [03:58<00:00,  1.22it/s]\n",
      "val_epoch: 39/300 Step: 293 loss : 1.0562 accuracy: 0.5076: 100%|██████████| 33/33 [00:04<00:00,  7.18it/s]\n",
      "epoch: 40/300 Step: 293 loss : 1.0900 accuracy: 0.5599: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 40/300 Step: 293 loss : 1.0575 accuracy: 0.4924: 100%|██████████| 33/33 [00:04<00:00,  7.06it/s]\n",
      "epoch: 41/300 Step: 293 loss : 1.0849 accuracy: 0.5642: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 41/300 Step: 293 loss : 1.0562 accuracy: 0.5000: 100%|██████████| 33/33 [00:04<00:00,  7.12it/s]\n",
      "epoch: 42/300 Step: 293 loss : 1.0777 accuracy: 0.5565: 100%|██████████| 292/292 [03:59<00:00,  1.22it/s]\n",
      "val_epoch: 42/300 Step: 293 loss : 1.0565 accuracy: 0.5000: 100%|██████████| 33/33 [00:04<00:00,  7.10it/s]\n",
      "epoch: 43/300 Step: 293 loss : 1.0808 accuracy: 0.5728: 100%|██████████| 292/292 [03:57<00:00,  1.23it/s]\n",
      "val_epoch: 43/300 Step: 293 loss : 1.0578 accuracy: 0.4848: 100%|██████████| 33/33 [00:04<00:00,  7.59it/s]\n",
      "epoch: 44/300 Step: 293 loss : 1.0727 accuracy: 0.5651: 100%|██████████| 292/292 [03:58<00:00,  1.23it/s]\n",
      "val_epoch: 44/300 Step: 293 loss : 1.0542 accuracy: 0.5152: 100%|██████████| 33/33 [00:04<00:00,  7.10it/s]\n",
      "epoch: 45/300 Step: 293 loss : 1.0671 accuracy: 0.5967: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 45/300 Step: 293 loss : 1.0564 accuracy: 0.5076: 100%|██████████| 33/33 [00:04<00:00,  6.78it/s]\n",
      "epoch: 46/300 Step: 293 loss : 1.0736 accuracy: 0.5925: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 46/300 Step: 293 loss : 1.0605 accuracy: 0.4773: 100%|██████████| 33/33 [00:04<00:00,  7.49it/s]\n",
      "epoch: 47/300 Step: 293 loss : 1.0641 accuracy: 0.5676: 100%|██████████| 292/292 [04:04<00:00,  1.19it/s]\n",
      "val_epoch: 47/300 Step: 293 loss : 1.0600 accuracy: 0.4848: 100%|██████████| 33/33 [00:05<00:00,  6.52it/s]\n",
      "epoch: 48/300 Step: 293 loss : 1.0647 accuracy: 0.5899: 100%|██████████| 292/292 [04:05<00:00,  1.19it/s]\n",
      "val_epoch: 48/300 Step: 293 loss : 1.0621 accuracy: 0.4848: 100%|██████████| 33/33 [00:04<00:00,  7.00it/s]\n",
      "epoch: 49/300 Step: 293 loss : 1.0467 accuracy: 0.5976: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 49/300 Step: 293 loss : 1.0622 accuracy: 0.4848: 100%|██████████| 33/33 [00:04<00:00,  7.15it/s]\n",
      "epoch: 50/300 Step: 293 loss : 1.0565 accuracy: 0.6027: 100%|██████████| 292/292 [04:00<00:00,  1.22it/s]\n",
      "val_epoch: 50/300 Step: 293 loss : 1.0634 accuracy: 0.5000: 100%|██████████| 33/33 [00:04<00:00,  6.97it/s]\n",
      "epoch: 51/300 Step: 293 loss : 1.0515 accuracy: 0.6079: 100%|██████████| 292/292 [04:03<00:00,  1.20it/s]\n",
      "val_epoch: 51/300 Step: 293 loss : 1.0589 accuracy: 0.5000: 100%|██████████| 33/33 [00:04<00:00,  6.61it/s]\n",
      "epoch: 52/300 Step: 293 loss : 1.0557 accuracy: 0.6087: 100%|██████████| 292/292 [04:07<00:00,  1.18it/s]\n",
      "val_epoch: 52/300 Step: 293 loss : 1.0550 accuracy: 0.5152: 100%|██████████| 33/33 [00:04<00:00,  7.04it/s]\n",
      "epoch: 53/300 Step: 293 loss : 1.0518 accuracy: 0.6002: 100%|██████████| 292/292 [04:03<00:00,  1.20it/s]\n",
      "val_epoch: 53/300 Step: 293 loss : 1.0579 accuracy: 0.5000: 100%|██████████| 33/33 [00:04<00:00,  7.22it/s]\n",
      "epoch: 54/300 Step: 293 loss : 1.0432 accuracy: 0.6113: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 54/300 Step: 293 loss : 1.0621 accuracy: 0.4924: 100%|██████████| 33/33 [00:04<00:00,  7.42it/s]\n",
      "epoch: 55/300 Step: 293 loss : 1.0536 accuracy: 0.6010: 100%|██████████| 292/292 [04:05<00:00,  1.19it/s]\n",
      "val_epoch: 55/300 Step: 293 loss : 1.0579 accuracy: 0.4924: 100%|██████████| 33/33 [00:04<00:00,  6.99it/s]\n",
      "epoch: 56/300 Step: 293 loss : 1.0446 accuracy: 0.5933: 100%|██████████| 292/292 [04:00<00:00,  1.22it/s]\n",
      "val_epoch: 56/300 Step: 293 loss : 1.0572 accuracy: 0.5152: 100%|██████████| 33/33 [00:04<00:00,  7.12it/s]\n",
      "epoch: 57/300 Step: 293 loss : 1.0416 accuracy: 0.6045: 100%|██████████| 292/292 [04:01<00:00,  1.21it/s]\n",
      "val_epoch: 57/300 Step: 293 loss : 1.0541 accuracy: 0.5000: 100%|██████████| 33/33 [00:05<00:00,  6.24it/s]\n",
      "epoch: 58/300 Step: 293 loss : 1.0423 accuracy: 0.6010: 100%|██████████| 292/292 [04:03<00:00,  1.20it/s]\n",
      "val_epoch: 58/300 Step: 293 loss : 1.0568 accuracy: 0.4924: 100%|██████████| 33/33 [00:05<00:00,  6.50it/s]\n",
      "epoch: 59/300 Step: 25 loss : 0.9388 accuracy: 0.7500:   8%|▊         | 24/292 [00:21<04:00,  1.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m cost\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# cost에 대한 backward 구함\u001b[39;00m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39msecond_step(zero_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m acc_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39macc\n\u001b[1;32m     37\u001b[0m train\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m300\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39mcount\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_loss\u001b[38;5;241m/\u001b[39mcount\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "d = datetime.datetime.now()\n",
    "now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "print(f'[deeplearning Start]')\n",
    "print(f'deeplearning Start Time : {now_time}')\n",
    "MIN_loss=5000\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "sig=nn.Sigmoid()\n",
    "val_acc_list=[]\n",
    "MIN_acc=0\n",
    "check_val=1000\n",
    "for epoch in range(300):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    model.train()\n",
    "    for x, y in train:\n",
    "        y = y.float().to(device)\n",
    "        count+=1\n",
    "        x=x.float().to(device)\n",
    "        enable_running_stats(model)\n",
    "        predict = model(x).to(device)\n",
    "        cost = criterion(predict.softmax(dim=1), y.argmax(dim=1)) # cost 구함\n",
    "        acc=accuracy(predict.argmax(dim=1),y.argmax(dim=1))\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        disable_running_stats(model)\n",
    "        predict = model(x).to(device)\n",
    "        cost = criterion(predict.softmax(dim=1), y.argmax(dim=1)) # cost 구함\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        running_loss += cost.item()\n",
    "        acc_loss+=acc\n",
    "        train.set_description(f\"epoch: {epoch+1}/{300} Step: {count+1} loss : {running_loss/count:.4f} accuracy: {acc_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "    train_acc_list.append((acc_loss/count).cpu().detach().numpy())  \n",
    "    val=tqdm(test_dataloader)\n",
    "    path_list=[]\n",
    "    model.eval()\n",
    "    val_count=0\n",
    "    val_running_loss = 0.0\n",
    "    val_acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x,y in val:\n",
    "            y = y.to(device).float()\n",
    "            val_count+=1\n",
    "            x=x.to(device).float()\n",
    "            predict = model(x).to(device)\n",
    "            cost = criterion(predict.softmax(dim=1), y.argmax(dim=1)) # cost 구함\n",
    "            acc=accuracy(predict.argmax(dim=1),y.argmax(dim=1))\n",
    "            val_running_loss+=cost.item()\n",
    "            val_acc_loss+=acc\n",
    "            val.set_description(f\"val_epoch: {epoch+1}/{300} Step: {count+1} loss : {val_running_loss/val_count:.4f} accuracy: {val_acc_loss/val_count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/val_count))\n",
    "        val_acc_list.append((val_acc_loss/val_count).cpu().detach().numpy())  \n",
    "        torch.save(model.state_dict(), '../../model/supine_mteg/attention_eff_'+str(epoch)+'.pt')\n",
    "torch.save(model.state_dict(), '../../model/supine_mteg/attention_eff50_MIL.pt')\n",
    "end = time.time()\n",
    "d = datetime.datetime.now()\n",
    "now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "print(f'deeplearning Time : {now_time}s Time taken : {start-end}')\n",
    "print(f'[deeplearning End]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../model/mteg/attention_eff50_MIL.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.softmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
