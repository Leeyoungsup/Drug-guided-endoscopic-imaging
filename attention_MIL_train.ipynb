{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import os\n",
    "import torchmetrics\n",
    "import timm\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=4\n",
    "image_count=50\n",
    "img_size=256\n",
    "tf = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 432/1035 [01:20<03:01,  3.32it/s]"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, id,image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "\n",
    "        self.label = label_list\n",
    "        self.id=id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_tensor=self.id[idx]\n",
    "        image_tensor = self.img_path[idx]\n",
    "    \n",
    "        label_tensor =  self.label[idx]\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "\n",
    "train_data=pd.read_csv('../../data/mteg_data/supine_frame/train_label.csv') \n",
    "file_path='../../data/mteg_data/supine_frame/train/'\n",
    "train_image_list=[]\n",
    "for i in range(len(train_data)):\n",
    "    file_name=train_data.loc[i]['File Name']\n",
    "    id=file_name[:file_name.find('_')]\n",
    "    train_image_list.append(file_path+id)\n",
    "label_data=pd.read_csv('../../data/mteg_data/supine_frame/label.csv')  \n",
    "train_label_list=[]\n",
    "train_id_list=[]\n",
    "train_image_tensor = torch.empty((len(train_image_list),image_count,3, img_size, img_size))\n",
    "for i in tqdm(range(len(train_image_list))):\n",
    "    folder_name=os.path.basename(train_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    train_id_list.append(folder_name)\n",
    "    train_label_list.append(label-1) \n",
    "    image_file_list = glob(train_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            train_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "            \n",
    "val_data=pd.read_csv('../../data/mteg_data/supine_frame/val_label.csv') \n",
    "file_path='../../data/mteg_data/supine_frame/val/'\n",
    "val_image_list=[]\n",
    "for i in range(len(val_data)):\n",
    "    file_name=val_data.loc[i]['File Name']\n",
    "    id=file_name[:file_name.find('_')]\n",
    "    val_image_list.append(file_path+id)\n",
    "label_data=pd.read_csv('../../data/mteg_data/supine_frame/label.csv')  \n",
    "val_label_list=[]\n",
    "val_id_list=[]\n",
    "val_image_tensor = torch.empty((len(val_image_list),image_count,3, img_size, img_size))\n",
    "for i in tqdm(range(len(val_image_list))):\n",
    "    folder_name=os.path.basename(val_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    val_id_list.append(folder_name)\n",
    "    val_label_list.append(label-1) \n",
    "    image_file_list = glob(val_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            val_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            val_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            val_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "\n",
    "test_data=pd.read_csv('../../data/mteg_data/supine_frame/test_label.csv') \n",
    "file_path='../../data/mteg_data/supine_frame/test/'\n",
    "test_image_list=[]\n",
    "for i in range(len(test_data)):\n",
    "    file_name=test_data.loc[i]['File Name']\n",
    "    id=file_name[:file_name.find('_')]\n",
    "    test_image_list.append(file_path+id)\n",
    "label_data=pd.read_csv('../../data/mteg_data/supine_frame/label.csv')  \n",
    "test_label_list=[]\n",
    "test_id_list=[]\n",
    "test_image_tensor = torch.empty((len(test_image_list),image_count,3, img_size, img_size))\n",
    "for i in tqdm(range(len(test_image_list))):\n",
    "    folder_name=os.path.basename(test_image_list[i])\n",
    "    dst_label=label_data.loc[label_data['일련번호']==int(folder_name[:-1])]\n",
    "    dst_label=dst_label.loc[dst_label['구분값']==int(folder_name[-1])].reset_index()\n",
    "    label=int(dst_label.loc[0]['OTE 원인'])\n",
    "    test_id_list.append(folder_name)\n",
    "    test_label_list.append(label-1) \n",
    "    image_file_list = glob(test_image_list[i]+'/*.jpg')\n",
    "    if len(image_file_list)>image_count:\n",
    "        image_index = torch.randint(low=0, high=len(\n",
    "            image_file_list)-image_count, size=(1,))\n",
    "        count = 0\n",
    "        for index in range(image_count):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "    else:\n",
    "        count = 0\n",
    "        for index in range(len(image_file_list)):\n",
    "            image = 1-tf(Image.open(image_file_list[index]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "        for j in range(image_count-count):\n",
    "            image = 1-tf(Image.open(image_file_list[j]).resize((img_size,img_size)))\n",
    "            test_image_tensor[i,count] = image\n",
    "            count += 1\n",
    "\n",
    "train_id_list.extend(val_id_list)\n",
    "train_image_tensor = torch.concat((train_image_tensor,val_image_tensor))\n",
    "train_label_list.extend(val_label_list)\n",
    "train_dataset = CustomDataset(train_id_list,train_image_tensor, F.one_hot(torch.tensor(train_label_list).to(torch.int64)))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataset = CustomDataset(test_id_list,test_image_tensor, F.one_hot(torch.tensor(test_label_list).to(torch.int64)))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model('efficientnet_b2', pretrained=True)\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "class AttentionMILModel(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(AttentionMILModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "        self.dropout=torch.nn.Dropout(0.2)\n",
    "    def forward(self, inputs):\n",
    "        batch_size, num_tiles, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Flatten the inputs\n",
    "        inputs = inputs.view(-1, channels, height, width)\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size * num_tiles, 2048, 1, 1)\n",
    "        \n",
    "        # Reshape features\n",
    "        features = features.view(batch_size, num_tiles, -1)  # Shape: (batch_size, num_tiles, 2048)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(features)  # Shape: (batch_size, num_tiles, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)  # Normalize attention weights\n",
    "        \n",
    "        # Apply attention weights to features\n",
    "        attended_features = torch.sum(features * attention_weights, dim=1)  # Shape: (batch_size, 2048)\n",
    "        attended_features=self.dropout(attended_features)\n",
    "        attended_features=F.relu(attended_features)\n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(attended_features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups\n",
    "        \n",
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, _BatchNorm):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "            \n",
    "class_counts = [train_label_list.count(0), train_label_list.count(1), train_label_list.count(2)]\n",
    "class_weights = torch.tensor([sum(class_counts) / c for c in class_counts], dtype=torch.float)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights).to(device)\n",
    "Feature_Extractor=FeatureExtractor()\n",
    "model = AttentionMILModel(3,1408,Feature_Extractor)\n",
    "model = model.to(device)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "base_optimizer = torch.optim.SGD\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=2e-4, momentum=0.9)\n",
    "summary(model,(batch_size,image_count,3,img_size,img_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "d = datetime.datetime.now()\n",
    "now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "print(f'[deeplearning Start]')\n",
    "print(f'deeplearning Start Time : {now_time}')\n",
    "MIN_loss=5000\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "sig=nn.Sigmoid()\n",
    "val_acc_list=[]\n",
    "MIN_acc=0\n",
    "check_val=1000\n",
    "for epoch in range(300):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    model.train()\n",
    "    for x, y in train:\n",
    "        y = y.float().to(device)\n",
    "        count+=1\n",
    "        x=x.float().to(device)\n",
    "        enable_running_stats(model)\n",
    "        predict = model(x).to(device)\n",
    "        cost = criterion(predict, y) # cost 구함\n",
    "        acc=accuracy(predict.argmax(dim=1),y.argmax(dim=1))\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        disable_running_stats(model)\n",
    "        predict = model(x).to(device)\n",
    "        cost = criterion(predict, y) # cost 구함\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        running_loss += cost.item()\n",
    "        acc_loss+=acc\n",
    "        train.set_description(f\"epoch: {epoch+1}/{300} Step: {count+1} loss : {running_loss/count:.4f} accuracy: {acc_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "    train_acc_list.append((acc_loss/count).cpu().detach().numpy())  \n",
    "    val=tqdm(test_dataloader)\n",
    "    path_list=[]\n",
    "    model.eval()\n",
    "    val_count=0\n",
    "    val_running_loss = 0.0\n",
    "    val_acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x,y in val:\n",
    "            y = y.to(device).float()\n",
    "            val_count+=1\n",
    "            x=x.to(device).float()\n",
    "            predict = model(x).to(device)\n",
    "            cost = F.cross_entropy(predict.softmax(dim=1), y) # cost 구함\n",
    "            acc=accuracy(predict.argmax(dim=1),y.argmax(dim=1))\n",
    "            val_running_loss+=cost.item()\n",
    "            val_acc_loss+=acc\n",
    "            val.set_description(f\"val_epoch: {epoch+1}/{300} Step: {count+1} loss : {val_running_loss/val_count:.4f} accuracy: {val_acc_loss/val_count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/val_count))\n",
    "        val_acc_list.append((val_acc_loss/val_count).cpu().detach().numpy())  \n",
    "        torch.save(model.state_dict(), '../../model/supine_mteg/attention_eff_'+str(epoch)+'.pt')\n",
    "torch.save(model.state_dict(), '../../model/supine_mteg/attention_eff50_MIL.pt')\n",
    "end = time.time()\n",
    "d = datetime.datetime.now()\n",
    "now_time = f\"{d.year}-{d.month}-{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "print(f'deeplearning Time : {now_time}s Time taken : {start-end}')\n",
    "print(f'[deeplearning End]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../model/mteg/attention_eff50_MIL.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.softmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
